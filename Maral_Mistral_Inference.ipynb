{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m9Ut6l5D3KXp"
      },
      "outputs": [],
      "source": [
        "#@markdown # Installing The Libraries\n",
        "! pip install git+https://github.com/huggingface/transformers trl py7zr auto-gptq optimum accelerate peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Loading needed libraries\n",
        "\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import GenerationConfig\n",
        "from transformers import AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZZP46L_p3aWO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Loading the model\n",
        "\n",
        "model_name_or_id = \"MaralGPT/MaralGPT-Mistral-7B-v-0-1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-v0.1-GPTQ\")\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    model_name_or_id,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ww6JfY3E3tAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Prompt\n",
        "\n",
        "prompt = \"\\u06CC\\u06A9 \\u0628\\u0631\\u0646\\u0627\\u0645\\u0647 \\u0628\\u0627 \\u067E\\u0627\\u06CC\\u062A\\u0648\\u0646 \\u0628\\u0646\\u0648\\u06CC\\u0633\\u06CC\\u062F \\u06A9\\u0647 \\u06CC\\u06A9 \\u0639\\u062F\\u062F \\u0627\\u0632 \\u0648\\u0631\\u0648\\u062F\\u06CC \\u06AF\\u0631\\u0641\\u062A\\u0647\\u060C \\u0641\\u0627\\u06A9\\u062A\\u0648\\u0631\\u06CC\\u0644 \\u0622\\u0646 \\u0631\\u0627 \\u062D\\u0633\\u0627\\u0628 \\u06A9\\u0631\\u062F\\u0647 \\u0648 \\u062F\\u0631 \\u062E\\u0631\\u0648\\u062C\\u06CC \\u0686\\u0627\\u067E \\u06A9\\u0646\\u062F.\" #@param {type: \"string\"}\n",
        "prompt = f\"### Human: {prompt}\\n###Assistant:\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "khavWI3T3wEG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Generation settings (to be completed)\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    top_k=1,\n",
        "    temperature=0.5,\n",
        "    max_new_tokens=500,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YrWfw0bN4P74"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Results\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "#import time\n",
        "#st_time = time.time()\n",
        "outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "#print(time.time()-st_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "FRHDaf_B4as5",
        "outputId": "6c86e5e6-bbc1-4cc9-a81d-b7f0fa3f78e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Human: یک برنامه با پایتون بنویسید که یک عدد از ورودی گرفته، فاکتوریل آن را حساب کرده و در خروجی چاپ کند.\n",
            "###Assistant: در اینجا یک برنامه با پایتون وجود دارد که یک عدد از ورودی گرفته، فاکتوریل آن را حساب می کند و در خروجی چاپ می کند:```\n",
            "پایتون\n",
            "def factorial(n):\n",
            "    اگر n == 0:\n",
            "        فاکتوریل = 1\n",
            "    اگر n > 0:\n",
            "        فاکتوریل = n * factorial(n - 1)\n",
            "    بازگشت فاکتوریل\n",
            "نمایش فاکتوریل(5)\n",
            "```\n",
            "در این برنامه، یک فنکشن با نام \"factorial\" و یک ورودی عددی \"n\" ایجاد شده است. این فنکشن بررسی می کند که یک عدد از ورودی گرفته یا 0 است. اگر 0 باشد، فاکتوریل 1 را برمی گرداند. اگر بیشتر از 0 باشد، فاکتوریل را با کالک آن و فاکتوریل آن در حالت کم یک بر اساس فاکتوریل آن در حالت کم یک بر اساس فا\n"
          ]
        }
      ]
    }
  ]
}